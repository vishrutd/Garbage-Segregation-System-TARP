{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import os \n",
    "import numpy as np \n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import tflearn \n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d \n",
    "from tflearn.layers.core import input_data, dropout, fully_connected \n",
    "from tflearn.layers.estimator import regression \n",
    "import tensorflow as tf\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\images dataset\\\\TRAIN'\n",
    "TEST_DIR = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\images dataset\\\\TEST'\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = 'Garbage_Segregator-{}-{}.model'.format(LR, '6conv-basic') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(img, word_label): \n",
    "    if word_label == 'metal':\n",
    "            return [1, 0, 0, 0, 0] \n",
    "    elif word_label == 'paper':\n",
    "            return [0, 1, 0, 0, 0]\n",
    "    elif word_label == 'cardboard':\n",
    "            return [0, 0, 1, 0, 0]\n",
    "    elif word_label == 'glass':\n",
    "            return [0, 0, 0, 1, 0]\n",
    "    elif word_label == 'plastic':\n",
    "            return [0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = [] \n",
    "    for word_label in ['metal', 'paper', 'cardboard', 'glass', 'plastic']:\n",
    "            for img in tqdm(os.listdir(os.path.join(TRAIN_DIR, word_label))):\n",
    "                label = label_img(img, word_label)\n",
    "                path = os.path.join(TRAIN_DIR, word_label, img)\n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) \n",
    "                training_data.append([np.array(img), np.array(label)]) \n",
    "                img = load_img(path)\n",
    "                data = img_to_array(img)\n",
    "                samples = expand_dims(data, 0)\n",
    "                datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "                it = datagen.flow(samples, batch_size=1)\n",
    "                for i in range(9):\n",
    "                    batch = it.next()\n",
    "                    image = batch[0].astype('uint8')\n",
    "                    imgd = tf.image.encode_jpeg(image)\n",
    "                    #training_data.append([np.array(imgd), np.array(label)]\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(): \n",
    "    \n",
    "    testing_data = []\n",
    "    for word_label in ['metal', 'paper', 'cardboard', 'glass', 'plastic']:\n",
    "            for img in tqdm(os.listdir(os.path.join(TEST_DIR, word_label))): \n",
    "                path = os.path.join(TEST_DIR, word_label, img) \n",
    "                img_num = label_img(img, word_label) \n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
    "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) \n",
    "                testing_data.append([np.array(img), img_num]) \n",
    "    shuffle(testing_data) \n",
    "    np.save('test_data.npy', testing_data) \n",
    "    return testing_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 380/380 [00:36<00:00, 10.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 380/380 [00:58<00:00,  6.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 403/403 [01:24<00:00,  4.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 501/501 [03:30<00:00,  2.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 482/482 [04:06<00:00,  1.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [00:09<00:00,  8.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 80/80 [00:10<00:00,  7.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [00:12<00:00,  6.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 71/71 [00:13<00:00,  5.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 82/82 [00:18<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = create_train_data() \n",
    "test_data = process_test_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\layers\\core.py:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "convnet = input_data(shape =[None, IMG_SIZE, IMG_SIZE, 1], name ='input') \n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation ='relu') \n",
    "convnet = max_pool_2d(convnet, 5) \n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation ='relu') \n",
    "convnet = max_pool_2d(convnet, 5) \n",
    "\n",
    "convnet = conv_2d(convnet, 128, 5, activation ='relu') \n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation ='relu') \n",
    "convnet = max_pool_2d(convnet, 5) \n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation ='relu') \n",
    "convnet = max_pool_2d(convnet, 5) \n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation ='relu') \n",
    "convnet = dropout(convnet, 0.8) \n",
    "\n",
    "convnet = fully_connected(convnet, 5, activation ='softmax') \n",
    "convnet = regression(convnet, optimizer ='adam', learning_rate = LR, loss ='categorical_crossentropy', name ='targets') \n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir ='log') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data\n",
    "test = test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) \n",
    "Y = [i[1] for i in train] \n",
    "test_x = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) \n",
    "test_y = [i[1] for i in test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.75077\u001b[0m\u001b[0m | time: 5.875s\n",
      "| Adam | epoch: 025 | loss: 0.75077 - acc: 0.7520 -- iter: 2112/2146\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.72062\u001b[0m\u001b[0m | time: 7.059s\n",
      "| Adam | epoch: 025 | loss: 0.72062 - acc: 0.7565 | val_loss: 0.43392 - val_acc: 0.8363 -- iter: 2146/2146\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Admin\\Desktop\\TARP\\Garbage_Segregator-0.001-6conv-basic.model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.fit({'input': X}, {'targets': Y}, n_epoch = 25,validation_set =({'input': test_x}, {'targets': test_y}), \n",
    "           show_metric = True, run_id = MODEL_NAME) \n",
    "model.save(MODEL_NAME) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
